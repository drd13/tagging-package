{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate abundances with new decoder and FactorDis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tagging.paths import path_dataset\n",
    "from tagging.src.datasets import ApogeeDataset\n",
    "from tagging.src.networks import ConditioningAutoencoder,Embedding_Decoder,Feedforward,ParallelDecoder\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda = True if torch.cuda.is_available() else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 1000\n",
    "n_batch = 64\n",
    "n_z = 20\n",
    "n_cat = 30\n",
    "n_hidden = 10\n",
    "lr = 0.0001\n",
    "n_conditioned = 2\n",
    "loss_ratio = 10e-4\n",
    "lambda_gp = 10\n",
    "n_critic = 10\n",
    "lambda_fact = 10**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(path_dataset)\n",
    "dataset = ApogeeDataset(data[:50000],n_bins)\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "                                     batch_size = n_batch,\n",
    "                                     shuffle= True,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins+n_conditioned,512,128,32,n_z],activation=nn.SELU()).to(device)\n",
    "decoder = ParallelDecoder(n_bins=n_bins,n_hidden=n_hidden,n_latent=n_z+n_conditioned,activation=nn.SELU()).to(device)\n",
    "conditioning_autoencoder = ConditioningAutoencoder(encoder,decoder,n_bins=n_bins).to(device)\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.Adam(conditioning_autoencoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Feedforward([n_bins+n_z+n_conditioned,4096,1024,512,128,32,1],activation=nn.SELU()).to(device)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "zeros = torch.full((n_batch,2), 0.0, device=device)\n",
    "ones = torch.full((n_batch,2),1.0,device=device)\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batches_done = 0\n",
    "for epoch in range(10000):\n",
    "\n",
    "    for i, (x,u,v,idx) in enumerate(loader):\n",
    "        optimizer_G.zero_grad()\n",
    "        n_perm=torch.randperm(n_batch)\n",
    "        u_perm = u[n_perm]\n",
    "        n_perm2=torch.randperm(n_batch)\n",
    "\n",
    " \n",
    "        x_pred,z = conditioning_autoencoder(x,u[:,0:2])\n",
    "        x_perm,_ = conditioning_autoencoder(z,u_perm[:,0:2],train_encoder=False)\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "\n",
    "        fake = torch.cat((x_perm,z,u_perm[:,0:2]),1)\n",
    "        real = torch.cat((x,z,u[:,0:2]),1)\n",
    "\n",
    "        # Real images\n",
    "        real_validity = discriminator(real)\n",
    "        # Fake images\n",
    "        fake_validity = discriminator(fake)\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real.data, fake.data)\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake images\n",
    "            fake_validity = discriminator(fake)\n",
    "            real_validity = discriminator(real)\n",
    "\n",
    "            err_pred = loss(x_pred,x)\n",
    "\n",
    "            g_loss = err_pred-lambda_fact*(torch.mean(fake_validity)-torch.mean(real_validity))\n",
    "\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            if i%30*n_critic ==0:\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D: %f] [G: %f] [R: %f]\"\n",
    "                    % (epoch, 100, i, len(loader), d_loss.item(), torch.mean(fake_validity).item(),err_pred.item())\n",
    "                )\n",
    "\n",
    "        \n",
    "                batches_done += n_critic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(conditioning_autoencoder, \"conditional_parallel_decoder1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taggenv",
   "language": "python",
   "name": "taggenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
