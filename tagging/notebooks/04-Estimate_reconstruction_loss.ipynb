{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error estimation\n",
    "\n",
    "This notebook contains code that can be used to calculate the reconstruction error (MSE for example) of our different model types.\n",
    "\n",
    "\n",
    "The results here are calculated for an L2 loss. The loss can easily be changed to an L1 loss by switching ```loss = nn.MSELoss(reduce=False)\n",
    "``` to ```loss = nn.L1Loss(reduce=False)```\n",
    "\n",
    "\n",
    "Currently the notebook is set-up to evaluate the reconstruction on those stars ```idx_large``` in the dataset which have a large difference in physical parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Cannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import spatial\n",
    "import pickle\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "from tagging.paths import path_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_degree = 4\n",
    "data_file = path_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    "print(\"number of degrees of freedom:{}\".format(n_degree))\n",
    "\n",
    "################################################\n",
    "\n",
    "print(\"Loading data...\")\n",
    "#data_file = \"spectra_noiseless\"\n",
    "#data = pd.read_pickle(\"/share/rcifdata/ddm/flatiron/taggingPaper/data/final/train/{}.pd\".format(data_file))\n",
    "#data = pd.read_pickle(\"/share/rcifdata/ddm/flatiron/taggingPaper/data/final/train/spectra_noiseless.pd\")\n",
    "data = pd.read_pickle(data_file)\n",
    "#data_noisy = pd.read_pickle(opt.data_file)\n",
    "\n",
    "print(\"dataset is:{}\".format(data_file))\n",
    "\n",
    "\n",
    "\n",
    "#dataset = ApogeeDataset(data[:50000],n_bins)\n",
    "####################################################\n",
    "\n",
    "spectra_matrix = np.matrix(data[\"spectra\"].tolist())\n",
    "spectra_matrix = spectra_matrix[0:50000]\n",
    "\n",
    "params_list = data.params.tolist()\n",
    "params_list = params_list[0:50000]\n",
    "\n",
    "polynomial = PolynomialFeatures(degree=n_degree)\n",
    "params_matrix = polynomial.fit_transform(np.array(params_list))\n",
    "d = np.dot(np.linalg.inv(np.dot(params_matrix.T,params_matrix)),params_matrix.T)\n",
    "s= np.dot(d,spectra_matrix)\n",
    "\n",
    "fit_matrix = np.dot(params_matrix,s)\n",
    "res_matrix = spectra_matrix - fit_matrix\n",
    "\n",
    "swapped_matrix = fit_matrix[:25000]+res_matrix[25000:50000]\n",
    "print(fit_matrix)\n",
    "print(swapped_matrix)\n",
    "\n",
    "\n",
    "#loss = nn.MSELoss(reduce=False)\n",
    "loss = nn.L1Loss(reduce=False)\n",
    "real =torch.tensor(spectra_matrix[0:25000])\n",
    "fit =torch.tensor(fit_matrix[0:25000])\n",
    "swapped =torch.tensor(swapped_matrix[0:25000])\n",
    "err = loss(real,swapped)\n",
    "print(\"error swapping is {}\".format(np.mean(err.numpy())))\n",
    "#err = loss(real,fit)\n",
    "#print(\"error regular is {}\".format(np.mean(err.numpy())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at only matching stars constrained to have a signifcant difference in the T_eff and log_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = np.array(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_diff = params_list[0:25000]-params_list[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(params_diff[:,0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_large = np.argwhere(np.abs(params_diff[:,0])>500)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idxs_large = np.argwhere( (np.abs(params_diff[:,0])>500) & (np.abs(params_diff[:,1])>1.0) )[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = loss(real[idxs_large],swapped[idxs_large])\n",
    "print(\"error swapping is {}\".format(np.mean(err.numpy())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_matrix[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(params_list[0:10]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction FactorDis and FaderDis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import ntpath\n",
    "\n",
    "from tagging.src.datasets import ApogeeDataset\n",
    "from tagging.src.networks import ConditioningAutoencoder,Embedding_Decoder,Feedforward\n",
    "from tagging.src.utils import get_batch,invert_x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_large = np.hstack((idxs_large,idxs_large+25000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 7751 \n",
    "n_conditioned = 3\n",
    "n_z = 20\n",
    "architecture = \"fader\"\n",
    "n_batch = 100 #number of spectra in one batch\n",
    "batch_numbers = 60 #number of batches to use to calculate running average\n",
    "use_full_dataset = True\n",
    "\n",
    "data = pd.read_pickle(path_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_full_dataset:\n",
    "    data = data.iloc[idxs_large]\n",
    "    data = data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ApogeeDataset(data,n_bins = 7751)\n",
    "evaluation_loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "                                     batch_size = n_batch,\n",
    "                                     shuffle = False,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins+n_conditioned,2048,512,128,32,n_z],activation=nn.SELU()).to(device)\n",
    "decoder = Feedforward([n_z+n_conditioned,512,2048,8192,n_bins],activation=nn.SELU()).to(device)\n",
    "conditioning_autoencoder = ConditioningAutoencoder(encoder,decoder,n_bins=n_bins).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if architecture == \"fader\":\n",
    "    #conditioning_autoencoder.load_state_dict(torch.load(\"../../outputs/models/faderDis.save\"))\n",
    "    conditioning_autoencoder = torch.load(\"../../outputs/models/faderDiswFe.save\")\n",
    "elif architecture == \"factor\":\n",
    "    weights = torch.load(\"../../outputs/models/factorDis.save\")\n",
    "    try:\n",
    "        del weights['w.weight']\n",
    "    except:\n",
    "        pass\n",
    "    conditioning_autoencoder.load_state_dict(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_tot = 0\n",
    "errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = nn.MSELoss(reduce=False)\n",
    "loss = nn.L1Loss(reduce=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We alternate between measuring err_rec and err_swp to measure the reconstruction error and the reconstruction with swapping error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_numbers):\n",
    "    batch1 = get_batch(0+i*n_batch,n_batch,dataset)\n",
    "    batch2 = get_batch(int(data.shape[0]/2)+i*n_batch,n_batch,dataset)\n",
    "\n",
    "    x_test1,u_test1,v_test1,idx_test1 = batch1\n",
    "    x_test2,u_test2,v_test2,idx_test2 = batch2\n",
    "    _,z1 = conditioning_autoencoder(x_test1,u_test1[:,0:3],train_decoder=False)\n",
    "    _,z2 = conditioning_autoencoder(x_test2,u_test2[:,0:3],train_decoder=False)\n",
    "\n",
    "    x1_pred,_ = conditioning_autoencoder(z1,u_test1[:,0:3],train_encoder=False)\n",
    "    x1_pred_swp,_ = conditioning_autoencoder(z1,u_test2[:,0:3],train_encoder=False)\n",
    "    _,z1_pred = conditioning_autoencoder(x1_pred_swp,u_test2[:,0:3],train_decoder=False)\n",
    "\n",
    "    x1_pred_swp= invert_x(x1_pred_swp)\n",
    "    x1_pred= invert_x(x1_pred)\n",
    "    x_test2= invert_x(x_test2)\n",
    "    x_test1= invert_x(x_test1)\n",
    "    err_swp = loss(x1_pred_swp,x_test2) #err_swp is the error \n",
    "    err_rec = loss(x1_pred,x_test1)\n",
    "    err_tot+=err_swp.detach().cpu().float().mean()\n",
    "    errs.append(err_swp.detach().cpu().numpy())\n",
    "    #print(\"err_swp:{}\".format(err_swp))\n",
    "print(\"err_tot:{}\".format(err_tot*(1/batch_numbers)))\n",
    "errs = np.concatenate(errs)\n",
    "#errs = np.mean(errs,axis=1)\n",
    "print(errs.shape)\n",
    "print(\"err_tot, err:{}, std:{},unc:{}\".format(np.mean(errs),np.std(errs),np.std(errs)/np.sqrt(errs.shape[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
