{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate abundances with new decoder and FactorDis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tagging.paths import path_dataset\n",
    "from tagging.src.datasets import ApogeeDataset\n",
    "from tagging.src.networks import ConditioningAutoencoder,Embedding_Decoder,Feedforward,ParallelDecoder, Autoencoder,Discriminator\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda = True if torch.cuda.is_available() else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Dataset\n",
    "\n",
    "Because of the long run time, I will be working on a toy dataset for now.\n",
    "\n",
    "Here is the structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = 100000\n",
    "n_signals = 10\n",
    "n_bins = 200\n",
    "fraction_signal = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self,n_datapoints,n_signals,n_bins,fraction_signal):\n",
    "        \"\"\"\n",
    "        n_datapoints: int\n",
    "            number of datapoints in dataset\n",
    "        n_signals: int\n",
    "            number of laten variables to include in the signal\n",
    "        n_bins: int\n",
    "            number of bins in the latent signal\n",
    "        fraction_signal: int\n",
    "            fractions of bins containing each signal\"\"\"\n",
    "            \n",
    "        self.n_datapoints = n_datapoints\n",
    "        self.n_signals = n_signals\n",
    "        self.n_bins = n_bins\n",
    "        self.fraction_signal = fraction_signal\n",
    "    \n",
    "        self.basis = self.generate_signal_basis(self.n_signals,self.n_bins,self.fraction_signal)\n",
    "        self.amplitudes = self.generate_amplitudes_dataset(self.n_bins)\n",
    "        self.dataset = self.generate_dataset(self.basis,self.amplitudes)\n",
    "        self.dataset = torch.FloatTensor(self.dataset)\n",
    "\n",
    "        \n",
    "    def generate_signal_basis(self,n_signals,n_bins,fraction_signal):\n",
    "        \"\"\"returns an np.array of shape n_signals*n_bins where every row is the basis of its respective signal\"\"\"\n",
    "        return (np.random.random_sample(size=(n_signals,n_bins))<fraction_signal).astype(float)\n",
    "    \n",
    "    def generate_amplitudes_dataset(self,n_bins):\n",
    "        \"\"\"generate the amplitude of each observation signal bin\n",
    "        returns an array of shape n_signals*n_datapoints\"\"\"\n",
    "        return np.random.normal(0,0.3,size= (n_signals,n_datapoints))\n",
    "    \n",
    "    def generate_dataset(self,basis,amplitudes):\n",
    "        print(basis.shape)\n",
    "        print(amplitudes.shape)\n",
    "        return np.matmul(amplitudes.T,basis) \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.dataset[idx]\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ToyDataset(n_datapoints=n_datapoints,n_signals=n_signals,n_bins=n_bins,fraction_signal=fraction_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset.dataset[0])\n",
    "plt.plot(dataset.dataset[1])\n",
    "plt.plot(dataset.dataset[2])\n",
    "plt.plot(dataset.dataset[3])\n",
    "plt.plot(dataset.dataset[4])\n",
    "plt.xlim(0,50)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch setup\n",
    "\n",
    "We generate our neural network and setup our optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z = n_signals\n",
    "n_hidden = 10\n",
    "lr = 0.001\n",
    "n_batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins,128,32,n_z],activation=nn.LeakyReLU()).to(device)\n",
    "decoder = ParallelDecoder(n_bins=n_bins,n_hidden=n_hidden,n_latent=n_z,activation=nn.SELU()).to(device)\n",
    "autoencoder = Autoencoder(encoder,decoder,n_bins=n_bins).to(device)\n",
    "optimizer_autoencoder = torch.optim.Adam(autoencoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "                                     batch_size = n_batch,\n",
    "                                     shuffle= True,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vanilla autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "\n",
    "    for j,(x) in enumerate(loader):\n",
    "\n",
    "        optimizer_autoencoder.zero_grad()\n",
    "        x_pred,z = autoencoder(x.to(device))\n",
    "\n",
    "        err_pred = loss(x_pred,x.to(device))  \n",
    "        err_tot = err_pred\n",
    "\n",
    "        err_tot.backward()\n",
    "        optimizer_autoencoder.step()\n",
    "        if j%100==0:\n",
    "            print(f\"err:{err_tot}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train adverserialautoencoder\n",
    "\n",
    "We add a loss term to ensure the latent is gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins,128,32,n_z],activation=nn.LeakyReLU()).to(device)\n",
    "decoder = ParallelDecoder(n_bins=n_bins,n_hidden=n_hidden,n_latent=n_z,activation=nn.SELU()).to(device)\n",
    "autoencoder = Autoencoder(encoder,decoder,n_bins=n_bins).to(device)\n",
    "optimizer_G = torch.optim.Adam(autoencoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Feedforward([n_z,256,128,32,1],activation=nn.SELU()).to(device)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "lambda_gp = 10\n",
    "n_critic = 10\n",
    "lambda_fact = 10**-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.FloatTensor(np.random.random((real_samples.size(0), 1))).to(device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.FloatTensor(real_samples.shape[0], 1).fill_(1.0).to(device)\n",
    "    fake.requires_grad=False\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_done = 0\n",
    "for epoch in range(20000):\n",
    "\n",
    "    for j,(x) in enumerate(loader):\n",
    "\n",
    "        z_real = torch.randn(n_batch, n_z).to(device)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        x_pred,z_fake = autoencoder(x.to(device))\n",
    "        \n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        real_validity = discriminator(z_real)\n",
    "        fake_validity = discriminator(z_fake)\n",
    "        \n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, z_real.data, z_fake.data)\n",
    "        \n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            fake_validity = discriminator(z_fake)\n",
    "            real_validity = discriminator(z_real)\n",
    "\n",
    "            err_pred = loss(x_pred,x.to(device))\n",
    "\n",
    "            g_loss = err_pred-lambda_fact*(torch.mean(fake_validity)-torch.mean(real_validity))\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            if i%100*n_critic ==0:\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D: %f] [G: %f] [R: %f]\"\n",
    "                    % (epoch, 100, i, len(loader), d_loss.item(), torch.mean(fake_validity).item(),err_pred.item())\n",
    "                )\n",
    "\n",
    "        \n",
    "                batches_done += n_critic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numpy(z_real).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(to_numpy(z_real).flatten(),alpha=0.5)\n",
    "plt.hist(to_numpy(z_fake).flatten(),alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(to_numpy(z_real).flatten(),alpha=0.5)\n",
    "plt.hist(to_numpy(z_fake).flatten(),alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_real.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify latent\n",
    "\n",
    "We sparsify our autoencoders dependence on the latent. We create a vector ```switch``` containing the fraction of noise to replace the signal with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelSwitchDecoder(nn.Module):\n",
    "    def __init__(self,n_bins=100,n_hidden = 10,n_latent=22,activation=nn.LeakyReLU()):\n",
    "        super(ParallelSwitchDecoder, self).__init__()\n",
    "        self.n_bins = n_bins\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_latent = n_latent\n",
    "        self.n_output = 1\n",
    "        self.activation = activation\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.n_latent * self.n_bins, out_channels= self.n_hidden * self.n_bins, kernel_size=1, groups=self.n_bins)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.n_hidden * self.n_bins, out_channels= self.n_hidden * self.n_bins, kernel_size=1, groups=self.n_bins)\n",
    "        self.conv3 = nn.Conv1d(in_channels=self.n_hidden * self.n_bins, out_channels= self.n_output * self.n_bins, kernel_size=1, groups=self.n_bins)\n",
    "        \n",
    "        \n",
    "    def forward(self, latent, switch):\n",
    "        #print(f\"switch:{switch}\")\n",
    "        repeated_latent = latent.repeat(1,self.n_bins)\n",
    "        noise = torch.randn(n_batch,n_bins*n_z).to(device)\n",
    "        #print(repeated_latent.shape)\n",
    "        #print(noise.shape)\n",
    "        #print(switch.shape)\n",
    "\n",
    "        mixed_latent = noise*switch.sigmoid()+switch.sigmoid()*repeated_latent\n",
    "        mixed_latent = mixed_latent.unsqueeze(2)\n",
    "        #print(f\"repeated_latent:{repeated_latent.shape}\")\n",
    "\n",
    "        hidden1 = self.activation(self.conv1(mixed_latent))\n",
    "        #print(f\"hidden1:{hidden1.shape}\")\n",
    "\n",
    "        hidden2 = self.activation(self.conv2(hidden1))\n",
    "        #print(f\"hidden2:{hidden2.shape}\")\n",
    "\n",
    "        output = self.conv3(hidden2)\n",
    "        #print(f\"output:{output.shape}\")\n",
    "        output = torch.squeeze(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AutoencoderSwitch(nn.Module):\n",
    "    def __init__(self,encoder,decoder,n_bins = None):\n",
    "        super(AutoencoderSwitch, self).__init__()\n",
    "        self.n_bins = n_bins\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        \n",
    "    def forward(self, x,switch,train_encoder=True,train_decoder=True):\n",
    "        latent=None\n",
    "        output=None\n",
    "        if train_encoder:\n",
    "            x= self.encoder(x)\n",
    "            latent = x\n",
    "        if train_decoder:\n",
    "            x = self.decoder(x,switch)\n",
    "            output = x\n",
    "        return output,latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch = torch.randn(n_batch,n_bins*n_z).to(device)\n",
    "#switch = torch.sigmoid(switch)\n",
    "switch.requires_grad=True\n",
    "switch.shape\n",
    "optimizer_switch = torch.optim.Adam([switch], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins,128,32,n_z],activation=nn.LeakyReLU()).to(device)\n",
    "decoder = ParallelSwitchDecoder(n_bins=n_bins,n_hidden=n_hidden,n_latent=n_z,activation=nn.SELU()).to(device)\n",
    "autoencoder = AutoencoderSwitch(encoder,decoder,n_bins=n_bins).to(device)\n",
    "optimizer_G = torch.optim.Adam(autoencoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Feedforward([n_z,256,128,32,1],activation=nn.SELU()).to(device)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "lambda_gp = 10\n",
    "n_critic = 5\n",
    "lambda_fact = 10**-3\n",
    "lambda_switch = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_done = 0\n",
    "for epoch in range(20000):\n",
    "\n",
    "    for i,(x) in enumerate(loader):\n",
    "        z_real = torch.randn(n_batch, n_z).to(device)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        x_pred,z_fake = autoencoder(x.to(device),switch)\n",
    "        \n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        real_validity = discriminator(z_real)\n",
    "        fake_validity = discriminator(z_fake)\n",
    "        \n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, z_real.data, z_fake.data)\n",
    "        \n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_switch.zero_grad()\n",
    "\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            fake_validity = discriminator(z_fake)\n",
    "            real_validity = discriminator(z_real)\n",
    "\n",
    "            err_pred = loss(x_pred,x.to(device))\n",
    "            err_switch = torch.mean(switch.sigmoid())\n",
    "\n",
    "            \n",
    "            g_loss = err_pred-lambda_fact*(torch.mean(fake_validity)-torch.mean(real_validity))+lambda_switch*err_switch\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            optimizer_switch.step()\n",
    "\n",
    "            if i%10*n_critic ==0:\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D: %f] [G: %f] [R: %f] [S: %f]\"\n",
    "                    % (epoch, 100, i, len(loader), d_loss.item(), torch.mean(fake_validity).item(),err_pred.item(),err_switch.item())\n",
    "                )\n",
    "        \n",
    "                batches_done += n_critic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch.sigmoid()[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify with batchnorm\n",
    "\n",
    "Instead of adding a discriminator that enforces gaussianity we use a batchnorm layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelSwitchDecoder(nn.Module):\n",
    "    def __init__(self,n_bins=100,n_hidden = 10,n_latent=22,activation=nn.LeakyReLU()):\n",
    "        super(ParallelSwitchDecoder, self).__init__()\n",
    "        self.n_bins = n_bins\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_latent = n_latent\n",
    "        self.n_output = 1\n",
    "        self.activation = activation\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.n_latent * self.n_bins, out_channels= self.n_hidden * self.n_bins, kernel_size=1, groups=self.n_bins)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.n_hidden * self.n_bins, out_channels= self.n_hidden * self.n_bins, kernel_size=1, groups=self.n_bins)\n",
    "        self.conv3 = nn.Conv1d(in_channels=self.n_hidden * self.n_bins, out_channels= self.n_output * self.n_bins, kernel_size=1, groups=self.n_bins)\n",
    "        \n",
    "        \n",
    "    def forward(self, latent, switch):\n",
    "        #print(f\"switch:{switch}\")\n",
    "        repeated_latent = latent.repeat(1,self.n_bins)\n",
    "        noise = torch.randn(n_batch,n_bins*n_z).to(device)\n",
    "        #print(repeated_latent.shape)\n",
    "        #print(noise.shape)\n",
    "        #print(switch.shape)\n",
    "\n",
    "        mixed_latent = noise*switch.sigmoid()+switch.sigmoid()*repeated_latent\n",
    "        mixed_latent = mixed_latent.unsqueeze(2)\n",
    "        #print(f\"repeated_latent:{repeated_latent.shape}\")\n",
    "\n",
    "        hidden1 = self.activation(self.conv1(mixed_latent))\n",
    "        #print(f\"hidden1:{hidden1.shape}\")\n",
    "\n",
    "        hidden2 = self.activation(self.conv2(hidden1))\n",
    "        #print(f\"hidden2:{hidden2.shape}\")\n",
    "\n",
    "        output = self.conv3(hidden2)\n",
    "        #print(f\"output:{output.shape}\")\n",
    "        output = torch.squeeze(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AutoencoderBatchNormSwitch(nn.Module):\n",
    "    def __init__(self,encoder,decoder,n_bins = None):\n",
    "        super(AutoencoderBatchNormSwitch, self).__init__()\n",
    "        self.n_bins = n_bins\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.batchnorm_layer = nn.BatchNorm1d(n_z)\n",
    "\n",
    "        \n",
    "    def forward(self, x,switch,train_encoder=True,train_decoder=True):\n",
    "        latent=None\n",
    "        output=None\n",
    "        if train_encoder:\n",
    "            x= self.encoder(x)\n",
    "            x = self.batchnorm_layer(x)\n",
    "            latent = x\n",
    "        if train_decoder:\n",
    "            x = self.decoder(x,switch)\n",
    "            output = x\n",
    "        return output,latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch = torch.randn(n_batch,n_bins*n_z).to(device)\n",
    "#switch = torch.sigmoid(switch)\n",
    "switch.requires_grad=True\n",
    "switch.shape\n",
    "optimizer_switch = torch.optim.Adam([switch], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins,128,32,n_z],activation=nn.LeakyReLU()).to(device)\n",
    "decoder = ParallelSwitchDecoder(n_bins=n_bins,n_hidden=n_hidden,n_latent=n_z,activation=nn.SELU()).to(device)\n",
    "autoencoder = AutoencoderBatchNormSwitch(encoder,decoder,n_bins=n_bins).to(device)\n",
    "optimizer_autoencoder = torch.optim.Adam(autoencoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_switch = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "\n",
    "    for j,(x) in enumerate(loader):\n",
    "\n",
    "        optimizer_autoencoder.zero_grad()\n",
    "        optimizer_switch.zero_grad()\n",
    "\n",
    "        x_pred,z = autoencoder(x.to(device),switch)\n",
    "\n",
    "        err_rec = loss(x_pred,x.to(device))\n",
    "        err_switch = torch.mean(switch.sigmoid())\n",
    "\n",
    "        err_tot = err_rec+lambda_switch\n",
    "\n",
    "        err_tot.backward()\n",
    "        optimizer_autoencoder.step()\n",
    "        optimizer_switch.step()\n",
    "\n",
    "        if j%100==0:\n",
    "            print(f\"err_tot:{err_tot},err_rec:{err_rec},err_switch:{err_switch}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Training\n",
    "\n",
    "Here we present a method that uses the jacobian to train this neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Feedforward([n_bins,128,32,n_z],activation=nn.LeakyReLU()).to(device)\n",
    "decoder = Feedforward([n_z,32,128,n_bins],activation=nn.LeakyReLU()).to(device)\n",
    "autoencoder = Autoencoder(encoder,decoder,n_bins=n_bins).to(device)\n",
    "optimizer_autoencoder = torch.optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "optimizer_encoder = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "optimizer_decoder = torch.optim.Adam(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "                                     batch_size = n_batch,\n",
    "                                     shuffle= True,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_batch_decoder(z):\n",
    "    x_pred = decoder(z)\n",
    "    x_flatened = torch.mean(x_pred,dim=0)\n",
    "    return x_flatened\n",
    "\n",
    "def loss_jacobian(z):\n",
    "    #print(z.shape)\n",
    "    jac_z = jacobian(average_batch_decoder, z, create_graph=True)\n",
    "    #print(jac_z.shape)\n",
    "    jac_z_unbatched = torch.sum(jac_z,dim=1)\n",
    "    normalized_jac = torch.abs(jac_z_unbatched)/torch.sum(torch.abs(jac_z_unbatched),dim=1).unsqueeze(1).repeat(1,z.shape[1])\n",
    "    loss = torch.mean(torch.sum(normalized_jac**0.5,dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_batch_decoder(z):\n",
    "    x_pred = decoder(z)\n",
    "    x_flatened = torch.mean(x_pred,dim=0)\n",
    "    return x_flatened\n",
    "\n",
    "\"\"\"def loss_jacobian(z):\n",
    "    #print(z.shape)\n",
    "    jac_z = jacobian(average_batch_decoder, z, create_graph=True)\n",
    "    #print(jac_z.shape)\n",
    "    jac_z_unbatched = torch.sum(jac_z,dim=1)\n",
    "    normalized_jac = torch.abs(jac_z_unbatched)/torch.sum(torch.abs(jac_z_unbatched),dim=1).unsqueeze(1).repeat(1,z.shape[1])\n",
    "    loss = torch.mean(torch.sum(normalized_jac**0.5,dim=1))\n",
    "    return loss\n",
    "\"\"\"\n",
    "\n",
    "def loss_jacobian(z):\n",
    "    #print(z.shape)\n",
    "    jac_z = jacobian(average_batch_decoder, z, create_graph=True)\n",
    "    #print(jac_z.shape)\n",
    "    jac_z_unbatched = torch.sum(jac_z,dim=1)\n",
    "    #normalized_jac = torch.abs(jac_z_unbatched)/torch.sum(torch.abs(jac_z_unbatched),dim=1).unsqueeze(1).repeat(1,z.shape[1])\n",
    "    normalized_jac = jac_z_unbatched/torch.norm(jac_z_unbatched,p=2,dim=1).unsqueeze(1)\n",
    "    loss = torch.mean(torch.sum(torch.abs(normalized_jac),dim=1)) #stopped multiplying by 0.5\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_z = jacobian(average_batch_decoder, z, create_graph=True)\n",
    "jac_z_unbatched = torch.sum(jac_z,dim=1)\n",
    "jac_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.abs(jac_z_unbatched),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(jac_z_unbatched,p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(jac_z_unbatched)/torch.sum(torch.abs(jac_z_unbatched),dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "  for x in loader:\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    z = encoder(x.to(device))\n",
    "    x_pred = decoder(z)\n",
    "    #err_jacobian = loss_jacobian(z)\n",
    "    err_rec = recon_loss(x_pred,x.to(device))\n",
    "    #err = err_jacobian\n",
    "    err = err_rec#+0.0001*err_jacobian\n",
    "    #err = err_rec\n",
    "    err.backward()\n",
    "    optimizer_decoder.step()\n",
    "    optimizer_encoder.step()\n",
    "    print(\"err_jac:{},err_rec:{}\".format(err_jacobian,err_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numpy(x_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(to_numpy(x_pred)[0],c=\"orange\")\n",
    "plt.plot(to_numpy(x)[0],c=\"red\")\n",
    "plt.plot(to_numpy(x_pred)[2],c=\"blue\")\n",
    "plt.plot(to_numpy(x)[2],c=\"purple\")\n",
    "plt.xlim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[0]\n",
    "z = encoder(dataset[0].unsqueeze(0).to(device))\n",
    "x_pred1 = decoder(z)\n",
    "z[0,0]=1\n",
    "x_pred2 = decoder(z)\n",
    "z[0,0]=-0.5\n",
    "x_pred3 = decoder(z)\n",
    "\n",
    "plt.plot(to_numpy(x_pred1[0]))\n",
    "plt.plot(to_numpy(x_pred2[0]))\n",
    "plt.plot(to_numpy(x_pred3[0]))\n",
    "plt.plot(dataset.basis[0],label=\"basis\")\n",
    "plt.xlim(100,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[0]\n",
    "z = encoder(dataset[0].unsqueeze(0).to(device))\n",
    "x_pred1 = decoder(z)\n",
    "z[0,9]=1\n",
    "x_pred2 = decoder(z)\n",
    "z[0,9]=-0.5\n",
    "x_pred3 = decoder(z)\n",
    "\n",
    "plt.plot(to_numpy(x_pred1[0]))\n",
    "plt.plot(to_numpy(x_pred2[0]))\n",
    "plt.plot(to_numpy(x_pred3[0]))\n",
    "plt.plot(dataset.basis[0],label=\"basis\")\n",
    "plt.xlim(0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[0]\n",
    "z = encoder(dataset[0].unsqueeze(0).to(device))\n",
    "i=9\n",
    "x_pred1 = decoder(z)\n",
    "z[0,i]=0.5\n",
    "x_pred2 = decoder(z)\n",
    "z[0,i]=-0.5\n",
    "x_pred3 = decoder(z)\n",
    "\n",
    "plt.plot(to_numpy(x_pred1[0]))\n",
    "plt.plot(to_numpy(x_pred2[0]))\n",
    "plt.plot(to_numpy(x_pred3[0]))\n",
    "plt.plot(dataset.basis[1],label=\"basis\")\n",
    "plt.xlim(150,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing abundances vs learned features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(loader,encoder):\n",
    "    z_list = []\n",
    "    for x in loader:\n",
    "        z = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_list = []\n",
    "for i in range(100):\n",
    "    x = dataset[100*i:100*i+100]\n",
    "    z = encoder(x.to(device))\n",
    "    z_list.append(to_numpy(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_array = np.concatenate(z_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes =  dataset.amplitudes.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(10,10)\n",
    "for n_amplitude in range(10):\n",
    "    for n_z in range(10): \n",
    "        ax = fig.add_subplot(gs[n_amplitude, n_z])\n",
    "        ax.scatter(amplitudes[:1000,n_amplitude],z_array[:1000,n_z])\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "fig.suptitle(\"latent vs \\\"abundance\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(10,10)\n",
    "for n_amplitude in range(10):\n",
    "    for n_z in range(10): \n",
    "        ax = fig.add_subplot(gs[n_amplitude, n_z])\n",
    "        ax.scatter(amplitudes[:1000,n_amplitude],z_array[:1000,n_z])\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "fig.suptitle(\"latent vs \\\"abundance\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(amplitudes[:10000,0],z_array[:10000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(amplitudes[:10000,0],z_array[:10000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(amplitudes[:10000,0],z_array[:10000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(amplitudes[:10000,2],z_array[:10000,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(amplitudes[:10000,9],z_array[:10000,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(dataset.basis,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder(dataset[0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder(dataset[0].unsqueeze(0).to(device))\n",
    "x = to_numpy(decoder(z))[0]\n",
    "plt.plot(x)\n",
    "\n",
    "z[0,0]=0.1\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.1\")\n",
    "plt.plot()\n",
    "\n",
    "z[0,0]=0.2\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.2\")\n",
    "plt.plot()\n",
    "plt.xlim(0,50)\n",
    "\n",
    "\n",
    "z[0,0]=0.3\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.3\")\n",
    "plt.plot(dataset.basis[i],label=\"basis\")\n",
    "plt.legend()\n",
    "plt.xlim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder(dataset[0].unsqueeze(0).to(device))\n",
    "x = to_numpy(decoder(z))[0]\n",
    "plt.plot(x)\n",
    "\n",
    "z[0,1]=0.1\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.1\")\n",
    "plt.plot()\n",
    "\n",
    "z[0,1]=0.2\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.2\")\n",
    "plt.plot()\n",
    "plt.xlim(0,50)\n",
    "\n",
    "\n",
    "z[0,1]=0.3\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.3\")\n",
    "plt.plot(dataset.basis[8],label=\"basis\")\n",
    "plt.legend()\n",
    "plt.xlim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder(dataset[0].unsqueeze(0).to(device))\n",
    "x = to_numpy(decoder(z))[0]\n",
    "plt.plot(x)\n",
    "\n",
    "z[0,2]=0.1\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.1\")\n",
    "plt.plot()\n",
    "\n",
    "z[0,2]=0.2\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.2\")\n",
    "plt.plot()\n",
    "plt.xlim(0,50)\n",
    "\n",
    "\n",
    "z[0,2]=0.3\n",
    "plt.plot(to_numpy(decoder(z))[0],label=\"0.3\")\n",
    "plt.plot(dataset.basis[3],label=\"basis\")\n",
    "plt.legend()\n",
    "plt.xlim(100,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(dataset.basis[i])\n",
    "    plt.title(i)\n",
    "    plt.xlim(0,50)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taggenv",
   "language": "python",
   "name": "taggenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
